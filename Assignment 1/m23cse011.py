# -*- coding: utf-8 -*-
"""Deep Learning Assignment 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JD5zkixCA4uzZHQ3fXC_0kOata4yszN3
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""# Installing Libraries"""

!pip install idx2numpy

"""# Importing Libraries"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import matplotlib.pyplot as plt
import idx2numpy
from sklearn.metrics import confusion_matrix
import seaborn as sns

"""# Convert Idx files to Numpy array"""

def convert_idx_file(file_path):
    data = idx2numpy.convert_from_file(file_path)
    return data

"""# Load dataset files and converting to np array"""

df = '/content/drive/MyDrive/Deep-Learning-Assignments/MNIST/'

training_images = convert_idx_file('/content/drive/MyDrive/Deep-Learning-Assignments/MNIST/train-images.idx3-ubyte')
training_labels = convert_idx_file('/content/drive/MyDrive/Deep-Learning-Assignments/MNIST/train-labels.idx1-ubyte')

test_images = convert_idx_file('/content/drive/MyDrive/Deep-Learning-Assignments/MNIST/t10k-images.idx3-ubyte')
test_labels = convert_idx_file('/content/drive/MyDrive/Deep-Learning-Assignments/MNIST/t10k-labels.idx1-ubyte')

(training_images.shape)

"""# Normalize Images"""

train_images = training_images / 255.0
test_images = test_images / 255.0

train_images[0]

"""# Convert NumPy arrays to PyTorch tensors"""

train_images_tensor = torch.from_numpy(train_images.copy()).float()
train_labels_tensor = torch.from_numpy(training_labels.copy()).long()
test_images_tensor = torch.from_numpy(test_images.copy()).float()
test_labels_tensor = torch.from_numpy(test_labels.copy()).long()

(train_images_tensor.shape)

test_images_tensor.shape

train_images_tensor[0]

"""# Convolutional layers"""

conv1 = nn.Conv2d(1, 16, kernel_size=7, stride=1, padding=3)
conv2 = nn.Conv2d(16, 8, kernel_size=5, stride=1, padding=2)
conv3 = nn.Conv2d(8, 4, kernel_size=3, stride=2, padding=1)

"""# Define max pooling and average pooling"""

maxpool = nn.MaxPool2d(kernel_size=2)
avgpool = nn.AvgPool2d(kernel_size=2)

"""# Define the output layer"""

output_size = 10
fc = nn.Linear(4 * 2 * 2, output_size)

"""# Forward Propagation"""

def forward_pass(x):
    x = F.relu(maxpool(conv1(x)))
    x = F.relu(maxpool(conv2(x)))
    x = F.relu(avgpool(conv3(x)))
    x = x.view(x.size(0), -1)
    x = F.softmax(fc(x), dim=1)
    return x

"""# Prepare data loaders"""

print(test_images_tensor.shape)
print(test_labels_tensor.shape)

train_dataset = TensorDataset(train_images_tensor.unsqueeze(1), train_labels_tensor)
test_dataset = TensorDataset(test_images_tensor.unsqueeze(1), test_labels_tensor)

train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True)

"""# Define optimizer and loss function"""

all_parameters = list(conv1.parameters()) + list(conv2.parameters()) + list(conv3.parameters()) + list(fc.parameters())

optimizer = optim.Adam(params=all_parameters, lr=0.001)

criterion = nn.CrossEntropyLoss()

len(all_parameters)

optimizer

"""# Initialize lists to track loss and accuracy"""

epoch_losses = []
epoch_accuracies = []
epochs = 10
for epoch in range(epochs):
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = forward_pass(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = 100 * correct / total
    epoch_losses.append(epoch_loss)
    epoch_accuracies.append(epoch_accuracy)
    print(f"Epoch {epoch+1}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}%")

"""# Plotting Graphs"""

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(epoch_losses, label='Loss')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epoch_accuracies, label='Accuracy')
plt.title('Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

"""# Display some images from the dataset"""

num_images = 9



plt.figure(figsize=(5, 5))
for i in range(num_images):
    plt.subplot(3, 3, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images_tensor[i].reshape(28, 28), cmap=plt.cm.binary)
    plt.xlabel(train_labels_tensor[i].item())
plt.show()

"""# Trainable Parameters"""

trainable_params = sum(p.numel() for layer in [conv1, conv2, conv3, fc] for p in layer.parameters() if p.requires_grad)

trainable_params

true_labels = []
pred_labels = []
for images, labels in test_loader:
    outputs = forward_pass(images)
    _, predicted = torch.max(outputs.data, 1)
    true_labels.extend(labels.numpy())
    pred_labels.extend(predicted.numpy())

"""# Confusion matrix"""

cm = confusion_matrix(true_labels, pred_labels)

plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=range(10), yticklabels=range(10))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""---



---

# **Experiment 2**

# Define a mapping new classes
"""

new_class_map = {0: 0, 6: 0, 1: 1, 7: 1, 2: 2, 3: 2, 8: 2, 5: 2, 4: 3, 9: 3}

"""# Maping new training and testing labels"""

train_labels_tensor = torch.tensor([new_class_map[label.item()] for label in train_labels_tensor])
test_labels_tensor = torch.tensor([new_class_map[label.item()] for label in test_labels_tensor])

"""# FC  layer adjustment for 4 classes"""

output_size = 4
fc = nn.Linear(4 * 2 * 2, output_size)

"""# New Convolutional Layer"""

conv1 = nn.Conv2d(1, 16, kernel_size=7, stride=1, padding=3)
conv2 = nn.Conv2d(16, 8, kernel_size=5, stride=1, padding=2)
conv3 = nn.Conv2d(8, 4, kernel_size=3, stride=2, padding=1)
maxpool = nn.MaxPool2d(kernel_size=2)
avgpool = nn.AvgPool2d(kernel_size=2)

"""# Forward propagation function for new map"""

def forward_pass(x):
    x = F.relu(maxpool(conv1(x)))
    x = F.relu(maxpool(conv2(x)))
    x = F.relu(avgpool(conv3(x)))
    x = x.view(x.size(0), -1)
    x = F.softmax(fc(x), dim=1)
    return x

"""# Prepare data loaders"""

train_dataset = TensorDataset(train_images_tensor.unsqueeze(1), train_labels_tensor)
test_dataset = TensorDataset(test_images_tensor.unsqueeze(1), test_labels_tensor)

train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)

"""# Optimizer and loss function"""

all_parameters = list(conv1.parameters()) + list(conv2.parameters()) + list(conv3.parameters()) + list(fc.parameters())
optimizer = optim.Adam(params=all_parameters, lr=0.001)
criterion = nn.CrossEntropyLoss()

"""# Track loss and accuracy"""

epoch_losses = []
epoch_accuracies = []
epochs = 10
for epoch in range(epochs):
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = forward_pass(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()


    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = 100 * correct / total
    epoch_losses.append(epoch_loss)
    epoch_accuracies.append(epoch_accuracy)
    print(f"Epoch {epoch+1}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}%")

"""# New Confusion matrix"""

true_labels = []
pred_labels = []
for images, labels in test_loader:
    outputs = forward_pass(images)
    _, predicted = torch.max(outputs.data, 1)
    true_labels.extend(labels.numpy())
    pred_labels.extend(predicted.numpy())

cm = confusion_matrix(true_labels, pred_labels)

plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 5), yticklabels=range(1, 5))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""# Plotting loss and accuracy graph"""

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(epoch_losses, label='Loss')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()


plt.subplot(1, 2, 2)
plt.plot(epoch_accuracies, label='Accuracy')
plt.title('Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()